{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8HUNqGydSJq",
        "outputId": "0e69c309-22fe-4d37-aa08-6da3d31e654c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Rank 0] Starting training...\n",
            "Epoch 0 Loss: 0.1342\n",
            "Epoch 1 Loss: 0.0575\n",
            "Training completed in 44.54s on 1 GPU(s)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DataLoader, DistributedSampler\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "def setup(rank, world_size):\n",
        "    # Only initialize distributed if world_size > 1\n",
        "    if world_size > 1:\n",
        "        dist.init_process_group(\n",
        "            backend=\"nccl\",\n",
        "            init_method=\"tcp://127.0.0.1:29500\",\n",
        "            rank=rank,\n",
        "            world_size=world_size\n",
        "        )\n",
        "        torch.cuda.set_device(rank)\n",
        "\n",
        "def cleanup(world_size):\n",
        "    # Only destroy process group if world_size > 1\n",
        "    if world_size > 1:\n",
        "        dist.destroy_process_group()\n",
        "\n",
        "def train(rank, world_size):\n",
        "    print(f\"[Rank {rank}] Starting training...\")\n",
        "    setup(rank, world_size)\n",
        "\n",
        "    # Dataset & Sampler\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "\n",
        "    # Use DistributedSampler only if world_size > 1, otherwise use default sampler\n",
        "    if world_size > 1:\n",
        "        sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
        "        dataloader = DataLoader(dataset, batch_size=64, sampler=sampler)\n",
        "    else:\n",
        "        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "    # Model\n",
        "    model = models.resnet18(num_classes=10)\n",
        "    # Modify the first convolutional layer to accept 1 input channel for MNIST\n",
        "    model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "    model = model.cuda(rank) # Ensure model is on the correct GPU\n",
        "\n",
        "    # Use DDP only if world_size > 1\n",
        "    if world_size > 1:\n",
        "        ddp_model = DDP(model, device_ids=[rank])\n",
        "    else:\n",
        "        ddp_model = model # Use regular model when world_size is 1\n",
        "\n",
        "\n",
        "    # Loss & Optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(ddp_model.parameters(), lr=1e-3)\n",
        "\n",
        "    start = time.time()\n",
        "    for epoch in range(2):\n",
        "        # Set epoch for sampler only if it's a DistributedSampler\n",
        "        if world_size > 1:\n",
        "            sampler.set_epoch(epoch)\n",
        "        epoch_loss = 0.0\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.cuda(rank), y.cuda(rank)\n",
        "            optimizer.zero_grad()\n",
        "            output = ddp_model(X)\n",
        "            loss = loss_fn(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        if rank == 0:\n",
        "            print(f\"Epoch {epoch} Loss: {epoch_loss/len(dataloader):.4f}\")\n",
        "    end = time.time()\n",
        "\n",
        "    if rank == 0:\n",
        "        print(f\"Training completed in {end-start:.2f}s on {world_size} GPU(s)\")\n",
        "\n",
        "    cleanup(world_size)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    world_size = torch.cuda.device_count()\n",
        "    # Only use mp.spawn if world_size > 1\n",
        "    if world_size > 1:\n",
        "        mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
        "    else:\n",
        "        # Run training directly for single GPU\n",
        "        train(0, world_size) # Pass rank 0 and world_size 1"
      ]
    }
  ]
}